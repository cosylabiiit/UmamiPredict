{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavit21178/anaconda3/envs/my_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "print(\"hi\")\n",
    "from transformers import TFBertModel, BertTokenizer,BertConfig,BertModel\n",
    "import re\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 86.0/86.0 [00:00<00:00, 8.73kB/s]\n",
      "vocab.txt: 100%|██████████| 81.0/81.0 [00:00<00:00, 6.50kB/s]\n",
      "special_tokens_map.json: 100%|██████████| 112/112 [00:00<00:00, 44.3kB/s]\n",
      "config.json: 100%|██████████| 361/361 [00:00<00:00, 124kB/s]\n",
      "pytorch_model.bin: 100%|██████████| 1.68G/1.68G [02:27<00:00, 11.4MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load the ProtBert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"Rostlab/prot_bert_bfd\", do_lower_case=False)\n",
    "\n",
    "# Load the ProtBert model\n",
    "model = BertModel.from_pretrained(\"Rostlab/prot_bert_bfd\")\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "import pandas as pd\n",
    "df=pd.read_csv('merged_data_modified.csv')\n",
    "y=df['id']\n",
    "x=df['seq']\n",
    "\n",
    "#split the data into train and test\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(353,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#oversample the data to create a balanced dataset in train\n",
    "#use random oversampling\n",
    "print(x_train.shape)\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "x_train=x_train.values.reshape(-1,1)\n",
    "x_train, y_train = ros.fit_resample(x_train, y_train)\n",
    "x_train=x_train.reshape(-1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(494,) (494,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/pavit21178/anaconda3/envs/my_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "ids = tokenizer.batch_encode_plus(x_train, max_length=40, add_special_tokens=True, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "input_ids = ids['input_ids']\n",
    "attention_mask = ids['attention_mask']  \n",
    "\n",
    "# Get the hidden states of the BERT model\n",
    "outputs= model(input_ids, attention_mask=attention_mask)\n",
    "hidden_states = outputs.last_hidden_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([494, 40, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavit21178/anaconda3/envs/my_env/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89, 40, 1024])\n"
     ]
    }
   ],
   "source": [
    "#do same with test data\n",
    "ids = tokenizer.batch_encode_plus(x_test, max_length=40, add_special_tokens=True, pad_to_max_length=True, return_tensors=\"pt\")\n",
    "test_input_ids = ids['input_ids']\n",
    "test_attention_mask = ids['attention_mask']\n",
    "\n",
    "# Get the hidden states of the BERT model\n",
    "test_outputs= model(test_input_ids, attention_mask=test_attention_mask)\n",
    "test_hidden_states = test_outputs.last_hidden_state\n",
    "\n",
    "print(test_hidden_states.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([89, 40, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(test_hidden_states.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 01:28:20.935795: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "def build_CNN_classifier_model():\n",
    "    input = tf.keras.Input(shape=(40,1024)) # [batch_size, length, 1024].\n",
    "    #net = input # [batch_size, seq_length, 1024]\n",
    "\n",
    "\n",
    "    net = tf.keras.layers.Conv1D(128, (2), activation='relu', data_format='channels_last')(input)\n",
    "    net = tf.keras.layers.Conv1D(64, (1), activation='relu', data_format='channels_last')(net)\n",
    "\n",
    "    net = tf.keras.layers.GlobalMaxPool1D()(net)\n",
    "\n",
    "#    net = tf.keras.layers.Flatten()(net)\n",
    "\n",
    "    net = tf.keras.layers.Dense(32, activation=\"relu\")(net)\n",
    "\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)\n",
    "    net = tf.keras.layers.Dense(2, activation='softmax')(net)\n",
    "\n",
    "    return tf.keras.Model(input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 40, 1024)]        0         \n",
      "                                                                 \n",
      " conv1d_4 (Conv1D)           (None, 39, 128)           262272    \n",
      "                                                                 \n",
      " conv1d_5 (Conv1D)           (None, 39, 64)            8256      \n",
      "                                                                 \n",
      " global_max_pooling1d_2 (Glo  (None, 64)               0         \n",
      " balMaxPooling1D)                                                \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 272,674\n",
      "Trainable params: 272,674\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:Keras is training/fitting/evaluating on array-like data. Keras may not be optimized for this format, so if your input data format is supported by TensorFlow I/O (https://github.com/tensorflow/io) we recommend using that to load a Dataset instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m      4\u001b[0m model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/pavit21178/anaconda3/envs/my_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/home/pavit21178/anaconda3/envs/my_env/lib/python3.9/site-packages/tensorflow/python/framework/dtypes.py:829\u001b[0m, in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(type_value, _dtypes\u001b[38;5;241m.\u001b[39mDType):\n\u001b[1;32m    827\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m _INTERN_TABLE[type_value\u001b[38;5;241m.\u001b[39mas_datatype_enum]\n\u001b[0;32m--> 829\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot convert the argument `type_value`: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtype_value\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto a TensorFlow DType.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: Cannot convert the argument `type_value`: torch.float32 to a TensorFlow DType."
     ]
    }
   ],
   "source": [
    "#run the model\n",
    "model = build_CNN_classifier_model()\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit(hidden_states, y_train, epochs=10, batch_size=32, validation_data=(test_hidden_states, y_test))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Missing data for input \"input_2\". You passed a data dictionary with keys ['last_hidden_state', 'pooler_output']. Expected the following keys: ['input_2']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m bitter_classifier \u001b[38;5;241m=\u001b[39m build_CNN_classifier_model()\n\u001b[0;32m----> 2\u001b[0m output_test \u001b[38;5;241m=\u001b[39m \u001b[43mbitter_classifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_outputs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# tensor [batch_size, max_length, 1024]\u001b[39;00m\n\u001b[1;32m      3\u001b[0m output_test\n",
      "File \u001b[0;32m/home/pavit21178/anaconda3/envs/my_env/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/home/pavit21178/anaconda3/envs/my_env/lib/python3.9/site-packages/keras/engine/input_spec.py:197\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[0;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m names:\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m inputs:\n\u001b[0;32m--> 197\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    198\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing data for input \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    199\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou passed a data dictionary with keys \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    200\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    201\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected the following keys: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnames\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    202\u001b[0m         )\n\u001b[1;32m    203\u001b[0m     list_inputs\u001b[38;5;241m.\u001b[39mappend(inputs[name])\n\u001b[1;32m    204\u001b[0m inputs \u001b[38;5;241m=\u001b[39m list_inputs\n",
      "\u001b[0;31mValueError\u001b[0m: Missing data for input \"input_2\". You passed a data dictionary with keys ['last_hidden_state', 'pooler_output']. Expected the following keys: ['input_2']"
     ]
    }
   ],
   "source": [
    "bitter_classifier = build_CNN_classifier_model()\n",
    "output_test = bitter_classifier(test_outputs) # tensor [batch_size, max_length, 1024]\n",
    "output_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
